# RAG System Configuration

# Vector Database Settings
vector_db:
  provider: "chromadb"  # Options: chromadb for < 1M documents, faiss for larger datasets
  persist_directory: "./data/vectorstore"
  collection_name: "rag_documents"
  distance_metric: "cosine"  # Options: cosine (cosine similarity), l2 (Euclidean), ip (inner product)

# Embedding Model Settings
embeddings:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  model_kwargs:
    device: "auto"  # Options: cpu, cuda, mps, auto (auto will use GPU if available)
  encode_kwargs:
    normalize_embeddings: true
    batch_size: 32
  cache_folder: "./data/models"

# Document Processing Settings
document_processing:
  chunk_size: 1000 # Maximum number of characters per chunk
  chunk_overlap: 200 # Number of characters to overlap between chunks
  separators: ["\n\n", "\n", " ", ""]
  supported_formats: ["txt", "pdf", "md", "docx"]
  extract_metadata: true

# Local LLM Settings
llm:
  provider: "ollama"  # Options: ollama, llamacpp, gpt4all
  model_name: "llama3.2:latest"  # For Ollama: llama2, mistral, phi, codellama
  base_url: "http://localhost:11434"  # Ollama default
  temperature: 0.7
  max_tokens: 512
  streaming: true
  context_window: 4096

# Retrieval Settings
retrieval:
  top_k: 5
  score_threshold: 0.3
  use_reranking: false
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  max_context_length: 3000

# RAG Pipeline Settings
rag:
  prompt_template: |
    Use the following pieces of context to answer the question at the end.
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    
    Context:
    {context}
    
    Question: {question}
    
    Answer:
  include_sources: true
  return_source_documents: true

# Chat Interface Settings
chat:
  interface: "gradio"  # Options: gradio, streamlit, fastapi
  host: "127.0.0.1"
  port: 7860
  share: false
  enable_history: true
  max_history_length: 10

# Logging Settings
logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./logs/rag_system.log"

# Performance Settings
performance:
  enable_caching: true
  cache_ttl: 3600  # seconds
  max_workers: 4
  batch_processing: true
