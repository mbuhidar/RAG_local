# RAG-Based Chat Application with Local LLM

A Retrieval-Augmented Generation (RAG) chat application that runs entirely on local infrastructure, ensuring privacy and control over your data.

## GitHub Copilot Agent Skills

This project includes custom GitHub Copilot agent skills to accelerate development. Use the following triggers in your Copilot chat:

### Available Skills

- **@rag-arch** - Get help with RAG architecture and design decisions
- **@vector-db** - Assistance with vector database operations
- **@local-llm** - Local LLM integration and optimization
- **@doc-process** - Document processing pipeline development
- **@embeddings** - Embedding model selection and optimization
- **@rag-test** - Testing and evaluation strategies
- **@chat-ui** - Chat interface development
- **@optimize** - Performance optimization tips

### Usage Examples

```
# In GitHub Copilot Chat:
@rag-arch How should I chunk documents for optimal retrieval?
@vector-db Show me how to create a ChromaDB collection with metadata filtering
@local-llm What's the best way to load an Ollama model with streaming?
@embeddings Which embedding model works best for technical documentation?
```

## Project Structure

```
RAG_local/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ copilot-instructions.md    # Project-wide Copilot instructions
â”œâ”€â”€ .vscode/
â”‚   â”œâ”€â”€ settings.json               # VS Code settings
â”‚   â””â”€â”€ copilot-agent-skills.json   # Custom Copilot agent skills
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ embeddings/                 # Embedding generation
â”‚   â”œâ”€â”€ retrieval/                  # Vector search and retrieval
â”‚   â”œâ”€â”€ llm/                        # Local LLM integration
â”‚   â”œâ”€â”€ processing/                 # Document processing
â”‚   â””â”€â”€ chat/                       # Chat interface
â”œâ”€â”€ tests/                          # Unit and integration tests
â”œâ”€â”€ config/                         # Configuration files
â”œâ”€â”€ data/                          # Sample documents and datasets
â””â”€â”€ README.md
```

## Getting Started

1. **Install Dependencies** (to be created)
2. **Set Up Local LLM** (instructions coming)
3. **Configure Vector Database** (setup guide coming)
4. **Run the Application** (commands coming)

## Features (Planned)

- ğŸ“š Multi-format document ingestion (PDF, DOCX, TXT, MD)
- ğŸ” Semantic search with vector embeddings
- ğŸ¤– Local LLM inference (no cloud required)
- ğŸ’¬ Interactive chat interface
- ğŸ“Š Conversation history management
- âš¡ Optimized for consumer hardware
- ğŸ”’ Complete data privacy

## Technology Stack

- **LLM Framework**: LangChain / LlamaIndex
- **Vector Store**: ChromaDB / FAISS
- **Local LLM**: Ollama / llama.cpp
- **Embeddings**: sentence-transformers
- **Interface**: FastAPI + Gradio / Streamlit
- **Language**: Python 3.10+

## Development

Use the GitHub Copilot agent skills to accelerate development. The skills are configured to provide context-aware assistance specific to RAG applications.

## License

MIT License (or your preferred license)
